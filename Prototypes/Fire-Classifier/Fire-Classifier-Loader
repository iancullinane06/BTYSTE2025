import os
import csv
import numpy as np
import cv2
import tensorflow as tf
from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.utils.class_weight import compute_class_weight
from collections import Counter
from keras.applications import ResNet50
from keras.preprocessing.image import img_to_array
from keras.applications.resnet50 import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt
from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint
from dotenv import load_dotenv

load_dotenv()

# Hyperparameters and settings
Learning_Rate = 1e-5
width = height = 254
batchSize = 16
num_classes = 2
prototype = 5
num_epochs = 100  # Number of epochs to run
max_images_per_batch = 32  # Cap on number of images processed per batch

data_dir = os.getenv('FIRE-CLASSIFICATION-DATASET')

TrainFolder =  os.path.join(data_dir, "Training")
ListImageFolders = os.listdir(TrainFolder)
# Data augmentation settings
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,  # 20% of the data for validation
    horizontal_flip=True,
    zoom_range=0.1,
    shear_range=0.1,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1
)

# Transformations
def transformImg(img):
    img = cv2.resize(img, (height, width))
    img = img_to_array(img)
    img = preprocess_input(img)
    return img

# Read images and labels
def ReadImagePathsAndLabels():
    image_paths = []
    labels = []
    for folder in ListImageFolders:
        folder_path = os.path.join(TrainFolder, folder)
        for filename in os.listdir(folder_path):
            if filename.endswith(('.png', '.jpg', '.jpeg')):
                full_path = os.path.join(folder_path, filename)
                if os.path.isfile(full_path):
                    image_paths.append(full_path)
                    labels.append(1 if "fire" in folder.lower() else 0)
                else:
                    print(f"Warning: {full_path} is not a valid file.")
    return image_paths, labels

# Create and compile model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(height, width, 3))
x = base_model.output
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.5)(x)
predictions = tf.keras.layers.Dense(num_classes, activation='softmax')(x)

model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=Learning_Rate),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Create CSV files for loss and F1 score
loss_csv_file = "data/Fire-Prot-" + str(prototype) + "-loss.csv"
f1_csv_file = "data/Fire-Prot-" + str(prototype) + "-f1.csv"

# Initialize CSV files with headers
os.makedirs("data", exist_ok=True)
with open(loss_csv_file, "w", newline="") as file:
    db = csv.writer(file)
    db.writerow(["Loss"])

with open(f1_csv_file, "w", newline="") as file:
    db = csv.writer(file)
    db.writerow(["F1_Score"])

# Read image paths and labels
image_paths, labels = ReadImagePathsAndLabels()

# Check data balance
train_label_counts = Counter(labels)
print("Label distribution:", train_label_counts)

# Split the data into training and validation sets
train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=0.2, stratify=labels)

# Compute class weights
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)
class_weights = dict(enumerate(class_weights))
print("Class weights:", class_weights)

# Prepare DataFrame for generators
train_df = pd.DataFrame({'filename': train_paths, 'class': train_labels})
val_df = pd.DataFrame({'filename': val_paths, 'class': val_labels})

# Prepare training data generator
train_gen = datagen.flow_from_dataframe(
    dataframe=train_df,
    x_col='filename',
    y_col='class',
    target_size=(height, width),
    batch_size=batchSize,
    class_mode='raw',
    subset='training'
)

# Prepare validation data generator
val_gen = datagen.flow_from_dataframe(
    dataframe=val_df,
    x_col='filename',
    y_col='class',
    target_size=(height, width),
    batch_size=batchSize,
    class_mode='raw',
    subset='validation',
    shuffle=False
)

# Custom generator to cap number of images per batch
def capped_generator(generator, max_images):
    while True:
        images, labels = generator.next()
        if len(images) > max_images:
            images = images[:max_images]
            labels = labels[:max_images]
        yield images, labels

capped_train_gen = capped_generator(train_gen, max_images_per_batch)
capped_val_gen = capped_generator(val_gen, max_images_per_batch)

# TensorBoard callback
tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)

# Early stopping and model checkpointing
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

# Training the model using the capped generator and displaying the progress bar
steps_per_epoch = len(train_gen) if len(train_gen) < max_images_per_batch else max_images_per_batch
validation_steps = len(val_gen) if len(val_gen) < max_images_per_batch else max_images_per_batch

history = model.fit(capped_train_gen,
                    steps_per_epoch=steps_per_epoch,
                    validation_data=capped_val_gen,
                    validation_steps=validation_steps,
                    epochs=num_epochs,
                    class_weight=class_weights,
                    callbacks=[early_stopping, checkpoint, tensorboard_callback])

# Validate after training
val_predictions = []
val_true_labels = []

for val_images, val_labels in capped_val_gen:
    preds = model.predict(val_images)
    val_predictions.extend(np.argmax(preds, axis=1))
    val_true_labels.extend(val_labels)
    if len(val_true_labels) >= len(val_paths):
        break

# Compute confusion matrix
cm = confusion_matrix(val_true_labels, val_predictions)
print("Confusion Matrix:")
print(cm)

# Display confusion matrix
cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)
cm_display.plot()
plt.show()

# Save validation F1 score to CSV
val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')
with open(f1_csv_file, "a", newline="") as file:
    db = csv.writer(file)
    db.writerow([val_f1])

print("Final Validation F1 Score:", val_f1)

# Close TensorBoard callback
tensorboard_callback.on_train_end(None)
